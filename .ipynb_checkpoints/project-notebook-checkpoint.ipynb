{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64e041c0-4c9e-4640-9b19-0a7a3031a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import laserhockey.hockey_env as h_env\n",
    "import gymnasium as gym\n",
    "from importlib import reload\n",
    "import time\n",
    "import torch\n",
    "import DDPG\n",
    "import TD3\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb2cde42-002c-4b58-85d7-ca3dabbd4884",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5899e8f7-a6e5-4279-aed5-ef159928eced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenna\\anaconda3\\envs\\rl-proj\\lib\\site-packages\\gymnasium\\envs\\registration.py:693: UserWarning: \u001b[33mWARN: Overriding environment Hockey-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "C:\\Users\\lenna\\anaconda3\\envs\\rl-proj\\lib\\site-packages\\gymnasium\\envs\\registration.py:693: UserWarning: \u001b[33mWARN: Overriding environment Hockey-One-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'laserhockey.hockey_env' from 'C:\\\\Users\\\\lenna\\\\Desktop\\\\RL\\\\Project\\\\RL-Hockey\\\\laserhockey\\\\hockey_env.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "reload(h_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "108b196b-96dd-419f-8724-50158f36c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "### moving average to smooth out rewards ###\n",
    "def moving_average(data, win_size):\n",
    "    data = np.asarray(data)\n",
    "    averages = []\n",
    "    for i in range(len(data)-win_size):\n",
    "        averages.append(np.sum(data[i:i+win_size])/win_size)\n",
    "    return averages\n",
    "\n",
    "### opponent that performs random actions\n",
    "class Random_opponent():\n",
    "    def __init__(self, keep_mode=True):\n",
    "        self.keep_mode = keep_mode\n",
    "    def act(self, obs):\n",
    "        if self.keep_mode:\n",
    "            return np.random.uniform(-1,1,4)\n",
    "        return np.random.uniform(-1,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ba8f2d-b72a-4129-af47-f19185c9f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### function for saving train/test statistics\n",
    "def save_statistics(type, config, rewards, net_losses, wins, losses, winrate):\n",
    "    train_type = config[\"test\"]*\"test\" + (1-config[\"test\"])*\"train\"\n",
    "    with open(f'./results/{type}_hockey_{config[\"name\"]}_{config[\"mode\"]}_{train_type}_stats.pkl', 'wb') as f:\n",
    "        pickle.dump({\"Experiment setup\" : config, \"Rewards\": rewards, \"losses\": net_losses, \"wins\": wins, \"losses\":losses, \"winrate\": winrate}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0023afc2-9cc6-4527-b438-c65c55030fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### training and testing (if config[\"test\"] is set to True) ###\n",
    "def train_hockey(agent_type, agent1, agent2, config):\n",
    "    save_as1=f'./results/{agent_type}_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "    save_as2=f'./results/{agent_type}_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "    if config[\"mode\"]==\"normal\" or config[\"mode\"]==\"weak\" or config[\"mode\"]==\"selfplay\":\n",
    "        env = h_env.HockeyEnv()\n",
    "    elif config[\"mode\"]==\"defense\":\n",
    "        env = h_env.HockeyEnv(mode=h_env.HockeyEnv.TRAIN_DEFENSE)\n",
    "    elif config[\"mode\"]==\"attack\":\n",
    "        env = h_env.HockeyEnv(mode=h_env.HockeyEnv.TRAIN_SHOOTING)\n",
    "        \n",
    "    if config[\"mode\"]==\"normal\":\n",
    "        player2 = h_env.BasicOpponent(weak=False)\n",
    "    elif config[\"mode\"]==\"weak\":\n",
    "        player2 = h_env.BasicOpponent()\n",
    "    elif config[\"mode\"]==\"defense\" or config[\"mode\"]==\"attack\":\n",
    "        player2 = Random_opponent()\n",
    "    elif config[\"mode\"]==\"selfplay\":\n",
    "        player2 = agent2\n",
    "        \n",
    "    player1 = agent1\n",
    "    obs_agent2 = env.obs_agent_two()\n",
    "    if type(agent1).__name__==\"TD3Agent\":\n",
    "        train_losses = np.empty((0,4))\n",
    "    else:\n",
    "        train_losses = np.empty((0,2))\n",
    "    rewards = []\n",
    "    wins, losses, rewards = 0, 0, []\n",
    "    eps = 1 # entirely random actions for initial \n",
    "    desc = \"Training...\"\n",
    "    if config[\"test\"]:\n",
    "        desc=\"Testing...\"\n",
    "        eps = 0.0\n",
    "    for i in tqdm(range(config[\"episodes\"]), desc=desc, unit=\"episodes\", colour=\"green\"):\n",
    "        obs, info = env.reset()\n",
    "        d = False\n",
    "        ep_r = 0\n",
    "        old_r = 0\n",
    "        while not d:\n",
    "            if config[\"render\"]:\n",
    "                env.render()\n",
    "            a1 = player1.act(obs, eps=eps)\n",
    "            a2 = player2.act(obs_agent2)\n",
    "            obsnew, r, d, _, info = env.step(np.hstack([a1,a2]))\n",
    "            if info['winner']==1:\n",
    "                wins += 1\n",
    "            elif info['winner']==-1:\n",
    "                losses += 1\n",
    "            if not config[\"test\"]:\n",
    "                player1.store_transition((obs, a1, r, obsnew, d))\n",
    "                if config[\"prio_replay\"] and abs(r)>0:\n",
    "                   for k in range(5):\n",
    "                       player1.store_transition((obs, a1, r, obsnew, d))\n",
    "                if config[\"mode\"]==\"selfplay\":\n",
    "                    player2.store_transition((obs, a2, r, obsnew, d))\n",
    "            obs_agent2 = env.obs_agent_two()\n",
    "            #print(abs(obs-obsnew), abs(old_r-r))\n",
    "            obs=obsnew\n",
    "            ep_r +=r\n",
    "            #print(r)\n",
    "            old_r = r\n",
    "        if not config[\"test\"] and i>100:\n",
    "            eps = config[\"eps\"]\n",
    "            loss = player1.train(config[\"iter_fit\"])\n",
    "            train_losses = np.concatenate((train_losses, np.asarray(loss)))\n",
    "            #if config[\"mode\"]==\"selfplay\":\n",
    "                #_ = player2.train(config[\"iter_fit\"])\n",
    "        '''if (i+1)%500==0:\n",
    "            config[\"learning_rate_critic\"] = config[\"learning_rate_critic\"]*0.5\n",
    "            config[\"learning_rate_actor\"] = config[\"learning_rate_actor\"]*0.5'''\n",
    "        rewards.append(ep_r)\n",
    "    print(f'Wins: {wins}')\n",
    "    print(f'Losses: {losses}')\n",
    "    winrate = wins/max(1,losses)\n",
    "    print(f'W/L: {winrate}')\n",
    "    env.close()\n",
    "    save_statistics(agent_type, config, rewards, train_losses, wins, losses, winrate)\n",
    "    if not config[\"test\"]:\n",
    "        torch.save(player1.state(), save_as1)\n",
    "        #if config[\"mode\"]==\"selfplay\":\n",
    "            #torch.save(player2.state(), save_as2)       \n",
    "    return train_losses, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78044b2b-41c5-42a7-9496-f2f544d85846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gym(agent1, config):\n",
    "    save_as1=f'./results/{config[\"agent_type\"]}_{config[\"env_type\"]}_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "    player1 = agent1\n",
    "    train_losses = np.empty((0,4))\n",
    "    if config[\"agent_type\"] == \"DDPG\":\n",
    "        train_losses = np.empty((0,2))\n",
    "    rewards = []\n",
    "    if config[\"env_type\"] == \"walker\":\n",
    "        env = gym.make(\"BipedalWalker-v3\", hardcore=False)\n",
    "    if config[\"env_type\"] == \"pendulum\":\n",
    "        env = gym.make(\"Pendulum-v1\")\n",
    "    #eps = 1 # entirely random actions for initial \n",
    "    desc = \"Training...\"\n",
    "    eps = config[\"eps\"]\n",
    "    if config[\"test\"]:\n",
    "        desc=\"Testing...\"\n",
    "        eps = 0.0\n",
    "    for i in tqdm(range(config[\"episodes\"]), desc=desc, unit=\"episodes\", colour=\"green\"):\n",
    "        obs, info = env.reset()\n",
    "        d = False\n",
    "        steps = 0\n",
    "        ep_r = 0\n",
    "        while not d:\n",
    "            steps += 1\n",
    "            if config[\"render\"]:\n",
    "                env.render()\n",
    "            a1 = player1.act(obs, eps=eps)\n",
    "            obsnew, r, d, _, info = env.step(a1)\n",
    "            if not config[\"test\"]:\n",
    "                player1.store_transition((obs, a1, r, obsnew, d))\n",
    "                if config[\"prio_replay\"] and abs(r)>0:\n",
    "                   for k in range(5):\n",
    "                       player1.store_transition((obs, a1, r, obsnew, d))\n",
    "            obs=obsnew\n",
    "            ep_r +=r\n",
    "            #rewards.append(r)\n",
    "            if steps>1000:\n",
    "                break\n",
    "        if not config[\"test\"]:\n",
    "            eps = config[\"eps\"]\n",
    "            loss = player1.train(config[\"iter_fit\"])\n",
    "            train_losses = np.concatenate((train_losses, np.asarray(loss)))\n",
    "            #if config[\"mode\"]==\"selfplay\":\n",
    "                #_ = player2.train(config[\"iter_fit\"])\n",
    "        '''if (i+1)%500==0:\n",
    "            config[\"learning_rate_critic\"] = config[\"learning_rate_critic\"]*0.5\n",
    "            config[\"learning_rate_actor\"] = config[\"learning_rate_actor\"]*0.5'''\n",
    "        rewards.append(ep_r)\n",
    "    env.close()\n",
    "    save_statistics(config[\"agent_type\"], config, rewards, train_losses, wins=None, losses=None, winrate=None)\n",
    "    if not config[\"test\"]:\n",
    "        torch.save(player1.state(), save_as1)\n",
    "        #if config[\"mode\"]==\"selfplay\":\n",
    "            #torch.save(player2.state(), save_as2)       \n",
    "    return train_losses, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eae6168-9191-4a92-8684-57699eb6857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### initializes agents and executes training procedure ###\n",
    "def init_train(config):\n",
    "    agent_type = config[\"agent_type\"]\n",
    "    if config[\"env_type\"] == \"hockey\":\n",
    "        env = h_env.HockeyEnv()\n",
    "    else:\n",
    "        if config[\"env_type\"] == \"walker\":\n",
    "            env = gym.make(\"BipedalWalker-v3\", hardcore=False)\n",
    "        if config[\"env_type\"] == \"pendulum\":\n",
    "            env = gym.make(\"Pendulum-v1\")\n",
    "    # turn off the respective parts of TD3 to analyze separately\n",
    "    if agent_type == \"CDQ\":\n",
    "        config[\"smoothing_clip\"] = 0\n",
    "        config[\"update_policy_every\"] = 1\n",
    "    if agent_type == \"TPS\":\n",
    "        config[\"cdq\"] = False\n",
    "        config[\"update_policy_every\"] = 1\n",
    "    if agent_type == \"DPU\":\n",
    "        config[\"cdq\"] = False\n",
    "        config[\"smoothing_clip\"] = 0\n",
    "\n",
    "    if agent_type ==\"DDPG\":\n",
    "        agent1 = DDPG.DDPGAgent(env.observation_space, env.action_space, discount=config[\"discount\"], eps=config[\"eps\"],\n",
    "                              update_target_every=config[\"update_target_every\"], update_policy_every=config[\"update_policy_every\"], \n",
    "                              hidden_sizes_actor=config[\"hidden_sizes_actor\"],hidden_sizes_critic=config[\"hidden_sizes_critic\"],\n",
    "                              smoothing_std=config[\"smoothing_std\"], smoothing_clip=config[\"smoothing_clip\"],\n",
    "                              learning_rate_actor=config[\"learning_rate_actor\"], learning_rate_critic=config[\"learning_rate_critic\"])\n",
    "    else:\n",
    "        agent1 = TD3.TD3Agent(env.observation_space, env.action_space, discount=config[\"discount\"], eps=config[\"eps\"],\n",
    "                              update_target_every=config[\"update_target_every\"], update_policy_every=config[\"update_policy_every\"], \n",
    "                              hidden_sizes_actor=config[\"hidden_sizes_actor\"],hidden_sizes_critic=config[\"hidden_sizes_critic\"],\n",
    "                              smoothing_std=config[\"smoothing_std\"], smoothing_clip=config[\"smoothing_clip\"],\n",
    "                              learning_rate_actor=config[\"learning_rate_actor\"], learning_rate_critic=config[\"learning_rate_critic\"])\n",
    "    agent2 = None\n",
    "    if config[\"mode\"] == \"selfplay\":\n",
    "        agent2 = TD3.TD3Agent(agent_type, env.observation_space, env.action_space, discount=config[\"discount\"], eps=config[\"eps\"],\n",
    "                          update_target_every=config[\"update_target_every\"], update_policy_every=config[\"update_policy_every\"], \n",
    "                          hidden_sizes_actor=config[\"hidden_sizes_actor\"],hidden_sizes_critic=config[\"hidden_sizes_critic\"],\n",
    "                          smoothing_std=config[\"smoothing_std\"], smoothing_clip=config[\"smoothing_clip\"],\n",
    "                          learning_rate_actor=config[\"learning_rate_actor\"], learning_rate_critic=config[\"learning_rate_critic\"])\n",
    "    if config[\"checkpoint1\"]:\n",
    "        agent1.restore_state(torch.load(config[\"checkpoint1\"]))\n",
    "    if config[\"checkpoint2\"]:\n",
    "        agent2.restore_state(torch.load(config[\"checkpoint2\"]))\n",
    "    env.close() \n",
    "    if config[\"env_type\"]==\"hockey\":\n",
    "        print(\"train hock\")\n",
    "        losses_wea, rewards_wea = train_hockey(agent_type, agent1, agent2, config)\n",
    "    else:\n",
    "        losses_wea, rewards_wea = train_gym(agent1,config)\n",
    "    rewards_wea_avg = moving_average(rewards_wea, 100)\n",
    "    if not config[\"test\"]:\n",
    "        plt.figure(figsize=(3,2))\n",
    "        plt.plot(rewards_wea_avg)\n",
    "        plt.title(f'{type(agent1).__name__}_wea_{config[\"mode\"]}')\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(3,2))\n",
    "        plt.plot(moving_average(losses_wea[:,0],100))\n",
    "        plt.title(f'{type(agent1).__name__}_wea_{config[\"mode\"]}')\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(3,2))\n",
    "        plt.plot(moving_average(losses_wea[:,1],100))\n",
    "        plt.title(f'{type(agent1).__name__}_wea_{config[\"mode\"]}')\n",
    "        plt.show()\n",
    "        if not agent_type ==\"DDPG\":\n",
    "            plt.figure(figsize=(3,2))\n",
    "            plt.plot(moving_average(losses_wea[:,2][losses_wea[:,2] != np.array(None)],100))\n",
    "            plt.title(f'{type(agent1).__name__}_wea_{config[\"mode\"]}')\n",
    "            plt.show()\n",
    "            plt.figure(figsize=(3,2))\n",
    "            plt.plot(moving_average(losses_wea[:,3][losses_wea[:,3] != np.array(None)],100))\n",
    "            plt.title(f'{type(agent1).__name__}_wea_{config[\"mode\"]}')\n",
    "            plt.show()\n",
    "    \n",
    "    '''checkpoint = f'./results/{type(agent).__name__}_hockey_{episodes}-eps{eps}-{mode}.pth' '''\n",
    "\n",
    "        \n",
    "    '''agent2 = DDPG.DDPGAgent(env.observation_space, env.action_space, \n",
    "                           discount=1, eps=eps, update_target_every=update_target_every, hidden_sizes_actor=hidden_sizes_actor,hidden_sizes_critic=hidden_sizes_critic)\n",
    "    agent2.restore_state(torch.load(checkpoint2))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb840b09-a9c9-41fe-94d2-bfab985004cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_config = {\n",
    "    \"name\" : \"name\",\n",
    "    \"agent_type\" : \"TD3\",\n",
    "    \"env_type\" : \"hockey\",\n",
    "    \"test\" : False,\n",
    "    \"render\" : False,\n",
    "    \"episodes\" : 10000,\n",
    "    \"mode\" : \"normal\",\n",
    "    \"eps\" : 0.1,\n",
    "    \"discount\":0.99,\n",
    "    \"update_target_every\":100,\n",
    "    \"update_policy_every\":2,\n",
    "    \"hidden_sizes_actor\" : [128,128],\n",
    "    \"hidden_sizes_critic\" : [128,128,64],\n",
    "    \"iter_fit\" : 20,\n",
    "    \"smoothing_std\"  : 0.0005,\n",
    "    \"smoothing_clip\" : 0.00025,\n",
    "    \"checkpoint1\" : None,\n",
    "    \"checkpoint2\" : None,\n",
    "    \"learning_rate_critic\": 0.001,\n",
    "    \"learning_rate_actor\": 0.001,\n",
    "    \"buffer_size\" : 250000,\n",
    "    \"theta\" : 0.01,\n",
    "    \"prio_replay\" : False\n",
    "}\n",
    "# lr of 0.0001 for both seems to work best\n",
    "# small buffer size seems to be better\n",
    "# eps 0.1 seems to be best\n",
    "# discount 1 shows best results (winning later isn't worse than earlier?)\n",
    "# 20 iterations with policy delay 2 best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15ad086e-18f8-4c3a-9776-0342dac04244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGENT CDQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  28%|\u001b[32m████████████████▋                                           \u001b[0m| 279/1000 [02:57<07:39,  1.57episodes/s]\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpendulum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menv_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_agent_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[43minit_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_hockey_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_agent.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[8], line 50\u001b[0m, in \u001b[0;36minit_train\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     48\u001b[0m     losses_wea, rewards_wea \u001b[38;5;241m=\u001b[39m train_hockey(agent_type, agent1, agent2, config)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m     losses_wea, rewards_wea \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gym\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m rewards_wea_avg \u001b[38;5;241m=\u001b[39m moving_average(rewards_wea, \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m, in \u001b[0;36mtrain_gym\u001b[1;34m(agent1, config)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     26\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m---> 27\u001b[0m a1 \u001b[38;5;241m=\u001b[39m \u001b[43mplayer1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m obsnew, r, d, _, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a1)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\Desktop\\RL\\Project\\RL-Hockey\\TD3.py:205\u001b[0m, in \u001b[0;36mTD3Agent.act\u001b[1;34m(self, observation, eps)\u001b[0m\n\u001b[0;32m    203\u001b[0m     eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eps\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m eps\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise()  \u001b[38;5;66;03m# action in -1 to 1 (+ noise)\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m#action = self.policy1.predict(observation) + eps*torch.normal(0,0.01,self.action_space.size)\u001b[39;00m\n\u001b[0;32m    207\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_space\u001b[38;5;241m.\u001b[39mlow \u001b[38;5;241m+\u001b[39m (action \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_space\u001b[38;5;241m.\u001b[39mhigh \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_space\u001b[38;5;241m.\u001b[39mlow)\n",
      "File \u001b[1;32m~\\Desktop\\RL\\Project\\RL-Hockey\\feedforward.py:26\u001b[0m, in \u001b[0;36mFeedforward.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\Desktop\\RL\\Project\\RL-Hockey\\feedforward.py:18\u001b[0m, in \u001b[0;36mFeedforward.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer,activation_fun \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations):\n\u001b[1;32m---> 18\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mactivation_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_activation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_activation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadout(x))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rl-proj\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rl-proj\\lib\\site-packages\\torch\\nn\\modules\\activation.py:98\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rl-proj\\lib\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for agent_type in [\"CDQ\", \"TPS\", \"DPU\", \"TD3\", \"DDPG\"]:\n",
    "    print(\"AGENT\", agent_type)\n",
    "    config = start_config.copy()\n",
    "    config[\"episodes\"] = 1000\n",
    "    config[\"agent_type\"] = agent_type\n",
    "    config[\"env_type\"] = \"pendulum\"\n",
    "    config[\"name\"] = f'env_{config[\"env_type\"]}_agent_{agent_type}'\n",
    "    init_train(config)\n",
    "    config[\"checkpoint1\"] = f'./results/{config[\"agent_type\"]}_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb5c2f2-bb32-46ab-8b60-a57a37cbe4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = start_config.copy()\n",
    "config[\"episodes\"] = 1000\n",
    "config[\"env_type\"] = \"walker\"\n",
    "config[\"name\"] = f\"walker\"\n",
    "config[\"mode\"] = \"normal\"\n",
    "agent_type=\"TD3\"\n",
    "init_train(config)\n",
    "'''config[\"checkpoint1\"] = f'./results/{agent_type}_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "# test agent\n",
    "config[\"mode\"] = \"normal\"\n",
    "config[\"test\"] = True\n",
    "#config[\"episodes\"] = 10\n",
    "#config[\"render\"] = True\n",
    "init_train(agent_type, env_type, config)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c6f2f7-9544-4f24-9d2c-0137d771fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = start_config.copy()\n",
    "config[\"episodes\"] = 1000\n",
    "config[\"agent_type\"] = \"DDPG\"\n",
    "config[\"env_type\"] = \"walker\"\n",
    "config[\"name\"] = f\"walker\"\n",
    "config[\"mode\"] = \"normal\"\n",
    "init_train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7998e05-2a09-4d13-9a9e-013f4406dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = start_config.copy()\n",
    "config[\"episodes\"] = 5000\n",
    "config[\"name\"] = f\"noslide\"\n",
    "#config[\"theta\"] = theta\n",
    "config[\"mode\"] = \"normal\"\n",
    "agent_type=\"TD3\"\n",
    "env_type=\"hockey\"\n",
    "init_train(config)\n",
    "config[\"checkpoint1\"] = f'./results/{config[\"agent_type\"]}_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "# test agent\n",
    "config[\"mode\"] = \"normal\"\n",
    "config[\"test\"] = True\n",
    "#config[\"episodes\"] = 10\n",
    "#config[\"render\"] = True\n",
    "init_train(agent_type, env_type, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ceafb3-868a-466d-a4f6-697208b2fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = start_config.copy()\n",
    "config[\"episodes\"] = 5000\n",
    "config[\"name\"] = f\"lrdecay\"\n",
    "config[\"mode\"] = \"normal\"\n",
    "agent_type=\"TD3\"\n",
    "env_type=\"hockey\"\n",
    "init_train(agent_type, env_type, config)\n",
    "config[\"checkpoint1\"] = f'./results/{agent_type}_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "# test agent\n",
    "config[\"mode\"] = \"normal\"\n",
    "config[\"test\"] = True\n",
    "#config[\"episodes\"] = 10\n",
    "#config[\"render\"] = True\n",
    "init_train(agent_type, env_type, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9885a20-f384-483c-ab15-35a8d735f46f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for pri in [True, False]:\n",
    "    config = start_config.copy()\n",
    "    config[\"episodes\"] = 5000\n",
    "    config[\"prio_replay\"] = pri\n",
    "    config[\"name\"] = f\"prio_{pri}\"\n",
    "    config[\"mode\"] = \"normal\"\n",
    "    agent_type=\"TD3\"\n",
    "    env_type=\"hockey\"\n",
    "    init_train(agent_type, env_type, config)\n",
    "    config[\"checkpoint1\"] = f'./results/{agent_type}_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "    # test agent\n",
    "    config[\"mode\"] = \"normal\"\n",
    "    config[\"test\"] = True\n",
    "    #config[\"episodes\"] = 10\n",
    "    #config[\"render\"] = True\n",
    "    init_train(agent_type, env_type, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f0115-76d8-4ea3-8f32-d471ca863cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pri = True\n",
    "config[\"name\"] = f\"prio_{pri}\"\n",
    "config[\"mode\"] = \"normal\"\n",
    "agent_type=\"TD3\"\n",
    "env_type=\"hockey\"\n",
    "config[\"checkpoint1\"] = f'./results/{agent_type}_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "env = h_env.HockeyEnv()\n",
    "agent1 = TD3.TD3Agent(env.observation_space, env.action_space, discount=config[\"discount\"], eps=config[\"eps\"],\n",
    "                              update_target_every=config[\"update_target_every\"], update_policy_every=config[\"update_policy_every\"], \n",
    "                              hidden_sizes_actor=config[\"hidden_sizes_actor\"],hidden_sizes_critic=config[\"hidden_sizes_critic\"],\n",
    "                              smoothing_std=config[\"smoothing_std\"], smoothing_clip=config[\"smoothing_clip\"],\n",
    "                              learning_rate_actor=config[\"learning_rate_actor\"], learning_rate_critic=config[\"learning_rate_critic\"])\n",
    "\n",
    "agent1.restore_state(torch.load(config[\"checkpoint1\"]))\n",
    "print(next(agent1.Q1.parameters())[0])\n",
    "print(next(agent1.Q_target1.parameters())[0])\n",
    "#for param in agent1.Q.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e2c51a-7556-4a0d-a4fa-c990cff8bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = \"TD3Agent\"\n",
    "env = \"hockey\"\n",
    "name = \"iter20_up_ev2\"\n",
    "mode = \"normal\"\n",
    "with (open(f\"results/{agent}_{env}_{name}_{mode}_train_stats.pkl\", \"rb\")) as openfile:\n",
    "    loaded_stats = pickle.load(openfile)\n",
    "print(loaded_stats[\"Experiment setup\"])\n",
    "rewards = loaded_stats[\"Rewards\"]\n",
    "plt.plot(moving_average(rewards,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a71a50-36ee-4dc7-bcc6-d1d61a63f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "### training without defense/attack ###\n",
    "\n",
    "for gamma in [0.99, 0.97, 0.95]:\n",
    "    config = start_config.copy()\n",
    "    config[\"name\"] = \"discount_new\"\n",
    "    config[\"discount\"] = gamma\n",
    "    config[\"episodes\"] = 5000\n",
    "    config[\"mode\"] = \"normal\"\n",
    "    init_train(config)\n",
    "    config[\"checkpoint1\"] = f'./results/TD3Agent_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "    # test agent\n",
    "    config[\"test\"] = True\n",
    "    config[\"episodes\"] = 5000\n",
    "    init_train(config)\n",
    "\n",
    "\n",
    "config = start_config.copy()\n",
    "for lr in [0.001, 0.0005, 0.0001, 0.00005, 0.00001]:\n",
    "    config = start_config.copy()\n",
    "    config[\"episodes\"] = 5000\n",
    "    config[\"learning_rate_critic\"] = lr\n",
    "    config[\"learning_rate_actor\"] = lr\n",
    "    #config[\"use_target_net\"] = False\n",
    "    config[\"name\"] = f\"lr_slidewin\"\n",
    "    config[\"mode\"] = \"normal\"\n",
    "    init_train(config)\n",
    "    config[\"checkpoint1\"] = f'./results/TD3Agent_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "    # test agent\n",
    "    config[\"mode\"] = \"normal\"\n",
    "    config[\"test\"] = True\n",
    "    #config[\"episodes\"] = 10\n",
    "    #config[\"render\"] = True\n",
    "    init_train(config)\n",
    "    config[\"test\"] = False\n",
    "\n",
    "\n",
    "'''for lr_crit in [0.001, 0.0001, 0.00001]:\n",
    "    config = start_config.copy()\n",
    "    config[\"name\"] = \"paramtest4\"\n",
    "    config[\"mode\"] = \"normal\"\n",
    "    print(\"LR CRITIC:\", lr_crit)\n",
    "    config[\"learning_rate_critic\"] = lr_crit\n",
    "    init_train(config)\n",
    "    config[\"checkpoint1\"] = f'./results/TD3Agent_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "    # test agent\n",
    "    config[\"mode\"] = \"normal\"\n",
    "    config[\"test\"] = True\n",
    "    init_train(config)\n",
    "    config[\"test\"] = False'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e0ca51-c508-4952-bd6a-d832508dcbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = h_env.HockeyEnv()\n",
    "agent1 = TD3.TD3Agent(env.observation_space, env.action_space, discount=config[\"discount\"], eps=config[\"eps\"],\n",
    "                          update_target_every=config[\"update_target_every\"], update_policy_every=config[\"update_policy_every\"], \n",
    "                          hidden_sizes_actor=config[\"hidden_sizes_actor\"],hidden_sizes_critic=config[\"hidden_sizes_critic\"],\n",
    "                          smoothing_std=config[\"smoothing_std\"], smoothing_clip=config[\"smoothing_clip\"],\n",
    "                          learning_rate_actor=config[\"learning_rate_actor\"], learning_rate_critic=config[\"learning_rate_critic\"])\n",
    "env.close()\n",
    "#print(agent1.Q1.state_dict)\n",
    "Q2 = agent1.Q_target1.parameters()\n",
    "for ii, param in enumerate(agent1.Q1.parameters()):\n",
    "    print(ii)\n",
    "    print(\"q1\",param)\n",
    "    with torch.no_grad():\n",
    "        param +=1\n",
    "    #print(next(Q2))\n",
    "for ii, param in enumerate(agent1.Q1.parameters()):\n",
    "    print(ii)\n",
    "    print(\"q1\",param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2bffc2-4fe0-40a6-964a-6bf7ff78a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selfplay\n",
    "config = start_config.copy()\n",
    "config[\"episodes\"] = 1000\n",
    "config[\"name\"] = \"20ktest\"\n",
    "for i in range(1):\n",
    "    if i>0:\n",
    "        config[\"mode\"] = \"selfplay\"\n",
    "    config[\"checkpoint1\"] = f'./results/TD3Agent_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "    config[\"checkpoint2\"] = f'./results/TD3Agent_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "    config[\"name\"] = f\"selfplay{i}\"\n",
    "    config[\"mode\"] = \"selfplay\"\n",
    "    config[\"test\"] = False\n",
    "    init_train(config)\n",
    "    config[\"test\"] = True\n",
    "    config[\"mode\"] = \"normal\"\n",
    "    config[\"checkpoint2\"] = None\n",
    "    init_train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2791e720-89e9-45d9-8c77-69c6b4463d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config[\"test\"] = True\n",
    "config[\"mode\"] = \"normal\"\n",
    "config[\"checkpoint2\"] = None\n",
    "init_train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e89d865-29b7-46a6-8425-a37fbd26c8da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### TRAINING CAMP ###\n",
    "config = start_config.copy()\n",
    "# defense training\n",
    "config[\"name\"] = \"traincamp_new\"\n",
    "config[\"mode\"] = \"defense\"\n",
    "config[\"episodes\"] = 1000\n",
    "init_train(config)\n",
    "config[\"checkpoint1\"] = f'./results/TD3Agent_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "# test agent trained on defense\n",
    "config[\"mode\"] = \"normal\"\n",
    "config[\"test\"] = True\n",
    "init_train(config)\n",
    "\n",
    "# shoot training\n",
    "config[\"mode\"] = \"attack\"\n",
    "config[\"test\"] = False\n",
    "init_train(config)\n",
    "config[\"checkpoint1\"] = f'./results/TD3Agent_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "# test agent trained on defense AND shooting\n",
    "config[\"mode\"] = \"normal\"\n",
    "config[\"test\"] = True\n",
    "init_train(config)\n",
    "\n",
    "# regular training\n",
    "config[\"mode\"] = \"normal\"\n",
    "config[\"test\"] = False\n",
    "init_train(config)\n",
    "config[\"checkpoint1\"] = f'./results/TD3Agent_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "# test agent trained on defense AND shooting\n",
    "config[\"mode\"] = \"normal\"\n",
    "config[\"test\"] = True\n",
    "init_train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56102694-5675-49ec-8110-1124c47b2146",
   "metadata": {},
   "outputs": [],
   "source": [
    "### test 0 eps vs 0.1 ###\n",
    "# test agent\n",
    "config = start_config.copy()\n",
    "config[\"name\"] = \"param_explore3\"\n",
    "config[\"mode\"] = \"normal\"\n",
    "#config[\"episodes\"] = 3000\n",
    "init_train(config)\n",
    "config[\"test\"] = True\n",
    "config[\"checkpoint1\"] = f'./results/TD3Agent_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "init_train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76401b23-3708-4d26-83e1-8956a75c8bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = True\n",
    "render = True\n",
    "episodes=10000\n",
    "mode = \"selfplay\"\n",
    "eps = 0.1\n",
    "update_target_every=100\n",
    "update_policy_every=20\n",
    "hidden_sizes_actor = [128,128]\n",
    "hidden_sizes_critic = [128,128,64]\n",
    "iter_fit = 32\n",
    "std  = 0.0005\n",
    "c = std/2\n",
    "\n",
    "\n",
    "env = h_env.HockeyEnv(mode=h_env.HockeyEnv.TRAIN_DEFENSE)\n",
    "agent = TD3.TD3Agent(env.observation_space, env.action_space, discount=1, eps=eps, \n",
    "                     update_target_every=update_target_every, update_policy_every=update_policy_every, \n",
    "                     hidden_sizes_actor=hidden_sizes_actor,hidden_sizes_critic=hidden_sizes_critic,\n",
    "                     smoothing_std=std, smoothing_clip=c)\n",
    "checkpoint = None\n",
    "if test:\n",
    "    checkpoint = f'./results/{type(agent).__name__}_hockey_{episodes}-eps{eps}-{mode}.pth'\n",
    "    episodes=1000\n",
    "    mode='normal'\n",
    "env.close()\n",
    "if checkpoint is not None:\n",
    "    agent.restore_state(torch.load(checkpoint))\n",
    "    \n",
    "#mode = \"normal\"\n",
    "losses_wea, rewards_wea = train(agent, mode=mode, episodes=episodes, eps=eps, test=test, iter_fit=iter_fit, render=render)\n",
    "rewards_wea_avg = moving_average(rewards_wea, 20)\n",
    "plt.plot(rewards_wea_avg)\n",
    "plt.title(f'{type(agent).__name__}_wea_{eps}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b65c806-2fd7-43c1-a93a-8cffa8f24e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mode='normal'\n",
    "episodes=10000\n",
    "env = h_env.HockeyEnv(mode=h_env.HockeyEnv.TRAIN_DEFENSE)\n",
    "agent = DDPG.DDPGAgent(env.observation_space, env.action_space, \n",
    "                       discount=1, eps=eps, update_target_every=update_target_every, hidden_sizes_actor=hidden_sizes_actor,hidden_sizes_critic=hidden_sizes_critic)\n",
    "checkpoint = None\n",
    "if test:\n",
    "    checkpoint = f'./results/{type(agent).__name__}_hockey_{episodes}-eps{eps}-{mode}.pth'\n",
    "    episodes=1000\n",
    "    mode='normal'\n",
    "env.close()\n",
    "if checkpoint is not None:\n",
    "losses_wea, rewards_wea = train(agent, mode=mode, episodes=episodes, eps=eps, test=test, iter_fit=iter_fit, render=render)\n",
    "rewards_wea_avg = moving_average(rewards_wea, 20)\n",
    "plt.plot(rewards_wea_avg)\n",
    "plt.title(f'{type(agent).__name__}_wea_{eps}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f982787-5cb6-407a-ae13-17d8c6d6eba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ed13bc-8d28-47aa-a66e-f54be6adc531",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(5)\n",
    "b = torch.clamp(A, 0.2,0.5)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e42456d-8689-4f40-a689-6b748b8fba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1000\n",
    "eps = 0\n",
    "checkpoint = f'./results/DDPG_hockey_{episodes}-eps{eps}-weak.pth'\n",
    "agent = DDPG.DDPGAgent(env.observation_space, env.action_space, discount=1, eps=eps)\n",
    "agent.restore_state(torch.load(checkpoint))\n",
    "losses_wea,  rewards_wea = train(agent, mode=\"weak\", episodes=1000, eps=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d4391f-66ab-4ce0-948f-86e502af8050",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_weak_avg = moving_average(rewards_wea, 20)\n",
    "\n",
    "plt.plot(rewards_weak_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d017caa-f8e3-47b8-b0a4-f0bd0bb1f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rewards_att_avg = moving_average(rewards_att, 20)\n",
    "plt.plot(rewards_def_avg.)\n",
    "plt.title(f'def_{eps}')\n",
    "plt.show()\n",
    "plt.plot(rewards_att_avg)\n",
    "plt.title(f'att_{eps}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3316118e-aa0a-47b2-b014-e3425ccad2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_def_avg = moving_average(rewards_def, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da5803e-e386-45b2-b86d-1cc4c4984060",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(rewards_def_avg)\n",
    "plt.title(f'schwanzus{eps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c88a23-ac98-419a-a63d-52c5089fd9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
