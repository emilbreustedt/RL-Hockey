{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains and tests an SCA agent on the Laser Hockey Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import laserhockey.hockey_env as h_env\n",
    "from importlib import reload\n",
    "from SAC import Agent\n",
    "import matplotlib.pyplot as plt\n",
    "# from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "n_games = 1000\n",
    "load_checkpoint = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilbreustedt/anaconda3/envs/RL/lib/python3.8/site-packages/gymnasium/envs/registration.py:693: UserWarning: \u001b[33mWARN: Overriding environment Hockey-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/Users/emilbreustedt/anaconda3/envs/RL/lib/python3.8/site-packages/gymnasium/envs/registration.py:693: UserWarning: \u001b[33mWARN: Overriding environment Hockey-One-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "# Initialization \n",
    "np.set_printoptions(suppress=True)\n",
    "reload(h_env)\n",
    "env = h_env.HockeyEnv(mode=h_env.HockeyEnv.NORMAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function\n",
    "def plot_learning_curve(x, scores, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.py:201: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  state = T.Tensor([observation]).to(self.actor.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... saving models ....\n",
      "episode  0 score -11.1 avg_score -11.1\n",
      "episode  1 score -29.3 avg_score -20.2\n",
      "episode  2 score -14.1 avg_score -18.2\n",
      "episode  3 score -26.4 avg_score -20.2\n",
      "episode  4 score -11.2 avg_score -18.4\n",
      "episode  5 score -11.5 avg_score -17.3\n",
      "episode  6 score -11.6 avg_score -16.4\n",
      "episode  7 score -34.2 avg_score -18.7\n",
      "episode  8 score -11.1 avg_score -17.8\n",
      "episode  9 score -32.1 avg_score -19.2\n",
      "episode  10 score -13.8 avg_score -18.8\n",
      "episode  11 score -31.4 avg_score -19.8\n",
      "episode  12 score 7.1 avg_score -17.7\n",
      "episode  13 score -48.4 avg_score -19.9\n",
      "episode  14 score -1.0 avg_score -18.7\n",
      "episode  15 score -43.4 avg_score -20.2\n",
      "episode  16 score -13.0 avg_score -19.8\n",
      "episode  17 score -30.5 avg_score -20.4\n",
      "episode  18 score -10.9 avg_score -19.9\n",
      "episode  19 score -30.1 avg_score -20.4\n",
      "episode  20 score -13.9 avg_score -20.1\n",
      "episode  21 score -36.7 avg_score -20.8\n",
      "episode  22 score -15.2 avg_score -20.6\n",
      "episode  23 score -37.7 avg_score -21.3\n",
      "episode  24 score -12.0 avg_score -20.9\n",
      "episode  25 score -33.7 avg_score -21.4\n",
      "episode  26 score -11.3 avg_score -21.1\n",
      "episode  27 score -29.9 avg_score -21.4\n",
      "episode  28 score -11.1 avg_score -21.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.ipynb Zelle 6\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m agent\u001b[39m.\u001b[39mremember(observation, action1, reward, observation_, done)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m agent\u001b[39m.\u001b[39;49mlearn()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m obs_opponent \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mobs_agent_two()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m observation \u001b[39m=\u001b[39m observation_\n",
      "File \u001b[0;32m~/Documents/GitHub/RL-Hockey/SAC.py:269\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m value_loss \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m F\u001b[39m.\u001b[39mmse_loss(value, value_target)\n\u001b[1;32m    268\u001b[0m value_loss\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 269\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    271\u001b[0m actions, log_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39msample_normal(state, reparameterize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    272\u001b[0m log_probs \u001b[39m=\u001b[39m log_probs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/RL/lib/python3.8/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/RL/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/RL/lib/python3.8/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    158\u001b[0m          grads,\n\u001b[1;32m    159\u001b[0m          exp_avgs,\n\u001b[1;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    162\u001b[0m          state_steps,\n\u001b[1;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/RL/lib/python3.8/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m func(params,\n\u001b[1;32m    214\u001b[0m      grads,\n\u001b[1;32m    215\u001b[0m      exp_avgs,\n\u001b[1;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    218\u001b[0m      state_steps,\n\u001b[1;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/anaconda3/envs/RL/lib/python3.8/site-packages/torch/optim/adam.py:263\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    262\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m--> 263\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    265\u001b[0m \u001b[39mif\u001b[39;00m capturable:\n\u001b[1;32m    266\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training of agent\n",
    "agent = Agent(input_dims=env.observation_space.shape, \n",
    "              env=env,\n",
    "              n_actions=int(env.action_space.shape[0]/2))\n",
    "\n",
    "opponent = h_env.BasicOpponent()\n",
    "\n",
    "# uncomment this line and do a mkdir tmp && mkdir video if you want to\n",
    "# record video of the agent playing the game.\n",
    "#env = wrappers.Monitor(env, 'tmp/video', video_callable=lambda episode_id: True, force=True)\n",
    "\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "\n",
    "if not load_checkpoint:\n",
    "    for i in range(n_games):\n",
    "        observation, info = env.reset()\n",
    "        obs_opponent = env.obs_agent_two()\n",
    "        \n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action1 = agent.act(observation)\n",
    "            action2 = opponent.act(obs_opponent)\n",
    "            \n",
    "            observation_, reward, done, _, info = env.step(np.hstack([action1,action2]))\n",
    "            score += reward\n",
    "            agent.remember(observation, action1, reward, observation_, done)\n",
    "            agent.learn()\n",
    "                \n",
    "            obs_opponent = env.obs_agent_two()\n",
    "            observation = observation_\n",
    "            \n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            if not load_checkpoint:\n",
    "                agent.save_models()\n",
    "\n",
    "        print('episode ', i, 'score %.1f' % score, 'avg_score %.1f' % avg_score)\n",
    "\n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    plot_learning_curve(x, score_history, 'SCA.png')\n",
    "\n",
    "else: \n",
    "    agent.load_models()\n",
    "    player1 = h_env.HumanOpponent(env=env, player=1)\n",
    "    player2 = agent\n",
    "    env.render()   \n",
    "    obs, info = env.reset()\n",
    "    obs_agent2 = env.obs_agent_two()\n",
    "    for _ in range(5000):\n",
    "        env.render()\n",
    "        a1 = player1.act(obs) \n",
    "        a2 = player2.act(obs_agent2)[4:]\n",
    "        obs, r, d, _, info = env.step(np.hstack([a1,a2]))    \n",
    "        obs_agent2 = env.obs_agent_two()\n",
    "        if d: break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
