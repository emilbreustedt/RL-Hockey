{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains and tests an SCA agent on the Laser Hockey Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import laserhockey.hockey_env as h_env\n",
    "from importlib import reload\n",
    "from SAC import Agent\n",
    "import matplotlib.pyplot as plt\n",
    "# from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "train_games = 500\n",
    "test_games = 100\n",
    "change_mode_after = 100\n",
    "load_checkpoint = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilbreustedt/anaconda3/envs/RL/lib/python3.8/site-packages/gymnasium/envs/registration.py:693: UserWarning: \u001b[33mWARN: Overriding environment Hockey-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/Users/emilbreustedt/anaconda3/envs/RL/lib/python3.8/site-packages/gymnasium/envs/registration.py:693: UserWarning: \u001b[33mWARN: Overriding environment Hockey-One-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "# Initialization \n",
    "np.set_printoptions(suppress=True)\n",
    "reload(h_env)\n",
    "\n",
    "# get enviroment\n",
    "env = h_env.HockeyEnv(mode=h_env.HockeyEnv.TRAIN_DEFENSE)\n",
    "\n",
    "#init agent\n",
    "agent = Agent(input_dims=env.observation_space.shape, \n",
    "              env=env,\n",
    "              n_actions=int(env.action_space.shape[0]/2))\n",
    "\n",
    "# and oponent\n",
    "opponent = h_env.BasicOpponent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function\n",
    "def plot_learning_curve(x, scores, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score -12.3 avg_score -12.3\n",
      "episode  1 score -12.2 avg_score -12.2\n",
      "episode  2 score 8.2 avg_score -5.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  3 score -17.5 avg_score -8.4\n",
      "episode  4 score -13.1 avg_score -9.4\n",
      "episode  5 score -13.3 avg_score -10.0\n",
      "episode  6 score -3.1 avg_score -9.0\n",
      "episode  7 score -14.3 avg_score -9.7\n",
      "episode  8 score -1.1 avg_score -8.7\n",
      "episode  9 score -20.9 avg_score -10.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.ipynb Zelle 6\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m agent\u001b[39m.\u001b[39mremember(observation, action1, reward, observation_, done)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mif\u001b[39;00m train_games \u001b[39m>\u001b[39m \u001b[39m10\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.ipynb#W5sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     agent\u001b[39m.\u001b[39;49mlearn()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.ipynb#W5sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m obs_opponent \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mobs_agent_two()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.ipynb#W5sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m observation \u001b[39m=\u001b[39m observation_\n",
      "File \u001b[0;32m~/Documents/GitHub/RL-Hockey/SAC.py:255\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m action \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mtensor(action, dtype\u001b[39m=\u001b[39mT\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    254\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(state)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 255\u001b[0m value_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_value(state_)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    256\u001b[0m value_[done] \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    258\u001b[0m actions, log_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39msample_normal(state, reparameterize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/RL/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/RL-Hockey/SAC.py:104\u001b[0m, in \u001b[0;36mValueNetwork.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[0;32m--> 104\u001b[0m     state_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(state)\n\u001b[1;32m    105\u001b[0m     state_value \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(state_value)\n\u001b[1;32m    106\u001b[0m     state_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(state_value)\n",
      "File \u001b[0;32m~/anaconda3/envs/RL/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/RL/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training of agent\n",
    "\n",
    "# uncomment this line and do a mkdir tmp && mkdir video if you want to\n",
    "# record video of the agent playing the game.\n",
    "#env = wrappers.Monitor(env, 'tmp/video', video_callable=lambda episode_id: True, force=True)\n",
    "\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "\n",
    "# conttinue from checkpoint\n",
    "if load_checkpoint:\n",
    "    agent.load_models()\n",
    "    \n",
    "for i in range(train_games):\n",
    "    \n",
    "    # set to normal mode after theshold\n",
    "    if i == change_mode_after: \n",
    "        env = h_env.HockeyEnv(mode=h_env.HockeyEnv.NORMAL)\n",
    "\n",
    "    observation, info = env.reset()\n",
    "    obs_opponent = env.obs_agent_two()\n",
    "    \n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        action1 = agent.act(observation)\n",
    "        action2 = opponent.act(obs_opponent)\n",
    "        \n",
    "        observation_, reward, done, _, info = env.step(np.hstack([action1,action2]))\n",
    "        reward = reward\n",
    "        score += reward\n",
    "        agent.remember(observation, action1, reward, observation_, done)\n",
    "        if train_games > 10:\n",
    "            agent.learn()\n",
    "            \n",
    "        obs_opponent = env.obs_agent_two()\n",
    "        observation = observation_\n",
    "        \n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if avg_score > best_score and i > 100:\n",
    "        best_score = avg_score\n",
    "        if not load_checkpoint:\n",
    "            agent.save_models()\n",
    "\n",
    "    print('episode ', i, 'score %.1f' % score, 'avg_score %.1f' % avg_score)\n",
    "\n",
    "x = [i+1 for i in range(train_games)]\n",
    "plot_learning_curve(x, score_history, 'SCA.png')\n",
    "'''\n",
    "else: \n",
    "    agent.load_models()\n",
    "    player1 = h_env.HumanOpponent(env=env, player=1)\n",
    "    player2 = agent\n",
    "    env.render()   \n",
    "    obs, info = env.reset()\n",
    "    obs_agent2 = env.obs_agent_two()\n",
    "    for _ in range(5000):\n",
    "        env.render()\n",
    "        a1 = player1.act(obs) \n",
    "        a2 = player2.act(obs_agent2)\n",
    "        obs, r, d, _, info = env.step(np.hstack([a1,a2]))    \n",
    "        obs_agent2 = env.obs_agent_two()\n",
    "        if d: break\n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... loading models ....\n",
      "Human Controls:\n",
      " left:\t\t\tleft arrow key left\n",
      " right:\t\t\tarrow key right\n",
      " up:\t\t\tarrow key up\n",
      " down:\t\t\tarrow key down\n",
      " tilt clockwise:\tw\n",
      " tilt anti-clockwise:\ts\n",
      " shoot :\tspace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.py:201: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  state = T.Tensor([observation]).to(self.actor.device)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.ipynb Zelle 7\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     env\u001b[39m.\u001b[39;49mrender() \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     action1 \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mact(observation)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emilbreustedt/Documents/GitHub/RL-Hockey/SAC.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     action2 \u001b[39m=\u001b[39m opponent\u001b[39m.\u001b[39mact(obs_opponent)\n",
      "File \u001b[0;32m~/Documents/GitHub/RL-Hockey/laserhockey/hockey_env.py:690\u001b[0m, in \u001b[0;36mHockeyEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    688\u001b[0m   pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[1;32m    689\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclock\u001b[39m.\u001b[39mtick(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata[\u001b[39m\"\u001b[39m\u001b[39mrender_fps\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 690\u001b[0m   pygame\u001b[39m.\u001b[39;49mdisplay\u001b[39m.\u001b[39;49mflip()\n\u001b[1;32m    691\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    692\u001b[0m   \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\n\u001b[1;32m    693\u001b[0m     np\u001b[39m.\u001b[39marray(pygame\u001b[39m.\u001b[39msurfarray\u001b[39m.\u001b[39mpixels3d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    694\u001b[0m   )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. Bitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. Klicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. Weitere Details finden Sie in Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "wins = 0\n",
    "agent.load_models()\n",
    "opponent = h_env.BasicOpponent()\n",
    "\n",
    "for i in range(test_games):\n",
    "    env.render() \n",
    "    observation, info = env.reset()\n",
    "    obs_opponent = env.obs_agent_two()\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render() \n",
    "        action1 = agent.act(observation)\n",
    "        action2 = opponent.act(obs_opponent)\n",
    "\n",
    "        observation, reward, done, _, info = env.step(np.hstack([action1,action2]))\n",
    "\n",
    "        obs_opponent = env.obs_agent_two()\n",
    "    if info['winner']==1:\n",
    "            wins+=1\n",
    "        \n",
    "wins"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
