{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e041c0-4c9e-4640-9b19-0a7a3031a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import laserhockey.hockey_env as h_env\n",
    "import gymnasium as gym\n",
    "from importlib import reload\n",
    "from TD3_helpers import *\n",
    "import time\n",
    "import torch\n",
    "import DDPG\n",
    "import TD3\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb2cde42-002c-4b58-85d7-ca3dabbd4884",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb840b09-a9c9-41fe-94d2-bfab985004cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_config = {\n",
    "    \"name\" : \"name\",\n",
    "    \"agent_type\" : \"TD3\",\n",
    "    \"env_type\" : \"hockey\",\n",
    "    \"test\" : False,\n",
    "    \"render\" : False,\n",
    "    \"episodes\" : 400,\n",
    "    \"mode\" : \"normal\",\n",
    "    \"eps\" : 0.1,\n",
    "    \"discount\":0.99,\n",
    "    \"update_target_every\":100,\n",
    "    \"update_policy_every\":2,\n",
    "    \"hidden_sizes_actor\" : [256,256],\n",
    "    \"hidden_sizes_critic\" : [256,256],\n",
    "    \"iter_fit\" : 1,\n",
    "    \"batch_size\" : 256,\n",
    "    \"smoothing_std\"  : 0.0001,\n",
    "    \"smoothing_clip\" : 0.0002,\n",
    "    \"checkpoint1\" : None,\n",
    "    \"checkpoint2\" : None,\n",
    "    \"learning_rate_critic\": 0.001,\n",
    "    \"learning_rate_actor\": 0.001,\n",
    "    \"buffer_size\" : int(1e6),\n",
    "    \"theta\" : 0.005,\n",
    "    \"prio_replay\" : False,\n",
    "    \"exp_phase\" : 0,\n",
    "    \"cdq\" : True\n",
    "}\n",
    "# lr of 0.0001 for both seems to work best for hockey\n",
    "# lr 0.001 for pendulum\n",
    "# iter_fit 40 for walker, 20 for rest\n",
    "# eps 0.1 seems to be best\n",
    "# discount 1 shows best results (winning later isn't worse than earlier?)\n",
    "# 20 iterations with policy delay 2 best for hockey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64339f0a-553e-4898-aab9-849c9a508749",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PENDULUM EXPERIMENT ###\n",
    "for agent_type in [\"TD3\", \"DDPG\", \"DPU\", \"TPS\", \"CDQ\"]:\n",
    "    print(\"AGENT\", agent_type)\n",
    "    config = start_config.copy()\n",
    "    config[\"discount\"] = 0.99\n",
    "    config[\"episodes\"] = 50\n",
    "    config[\"hidden_sizes_critic\"] = [32,32]\n",
    "    config[\"hidden_sizes_actor\"] = [32,32]\n",
    "    config[\"learning_rate_critic\"] = 0.001\n",
    "    config[\"learning_rate_actor\"] = 0.001\n",
    "    config[\"agent_type\"] = agent_type\n",
    "    config[\"env_type\"] = \"pendulum\"\n",
    "    config[\"name\"] = \"pendulum new\"\n",
    "    init_train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b35c8014-18ba-4b29-840a-7897db7b0c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGENT TD3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  48%|\u001b[32m██████████████████████████████                                \u001b[0m| 97/200 [24:53<26:25, 15.39s/episodes]\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheetah\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magentcompare\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 12\u001b[0m \u001b[43minit_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\RL\\Project\\RL-Hockey\\TD3_helpers.py:238\u001b[0m, in \u001b[0;36minit_train\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    236\u001b[0m     losses_wea, rewards_wea \u001b[38;5;241m=\u001b[39m train_hockey(agent_type, agent1, agent2, config)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 238\u001b[0m     losses_wea, rewards_wea \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gym\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m rewards_wea_avg \u001b[38;5;241m=\u001b[39m moving_average(rewards_wea, \u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\Desktop\\RL\\Project\\RL-Hockey\\TD3_helpers.py:85\u001b[0m, in \u001b[0;36mtrain_gym\u001b[1;34m(agent1, config)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m i\u001b[38;5;241m>\u001b[39mstart_train:\n\u001b[0;32m     84\u001b[0m     eps \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 85\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mplayer1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miter_fit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     train_losses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((train_losses, np\u001b[38;5;241m.\u001b[39masarray(loss)))\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m#if config[\"mode\"]==\"selfplay\":\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;66;03m#_ = player2.train(config[\"iter_fit\"])'\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\RL\\Project\\RL-Hockey\\TD3.py:290\u001b[0m, in \u001b[0;36mTD3Agent.train\u001b[1;34m(self, iter_fit)\u001b[0m\n\u001b[0;32m    288\u001b[0m q2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ2\u001b[38;5;241m.\u001b[39mQ_value(s, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy2\u001b[38;5;241m.\u001b[39mforward(s))\n\u001b[0;32m    289\u001b[0m actor_loss2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(q2)\n\u001b[1;32m--> 290\u001b[0m \u001b[43mactor_loss2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer2\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    292\u001b[0m actor_loss2 \u001b[38;5;241m=\u001b[39m actor_loss2\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rl-proj\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rl-proj\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### CHEETAH EXPERIMENT ###\n",
    "for agent_type in [ \"TD3\", \"DDPG\", \"DPU\", \"TPS\", \"CDQ\"]:\n",
    "    print(\"AGENT\", agent_type)\n",
    "    config = start_config.copy()\n",
    "    config[\"discount\"] = 0.99\n",
    "    config[\"episodes\"] = 200\n",
    "    config[\"learning_rate_critic\"] = 0.001\n",
    "    config[\"learning_rate_actor\"] = 0.001\n",
    "    config[\"agent_type\"] = agent_type\n",
    "    config[\"env_type\"] = \"cheetah\"\n",
    "    config[\"name\"] = \"agentcompare\"\n",
    "    init_train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee039449-2a54-4d5b-81f0-60ba85c8467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WALKER EXPERIMENT ###\n",
    "for agent_type in [\"DDPG\", \"DPU\", \"TPS\", \"CDQ\", \"TD3\"]:\n",
    "    print(\"AGENT\", agent_type)\n",
    "    config = start_config.copy()\n",
    "    config[\"episodes\"] = 500\n",
    "    config[\"learning_rate_critic\"] = 0.001\n",
    "    config[\"learning_rate_actor\"] = 0.001\n",
    "    config[\"agent_type\"] = agent_type\n",
    "    config[\"env_type\"] = \"walker\"\n",
    "    config[\"name\"] = \"agentcompare_long\"\n",
    "    init_train(config)\n",
    "    config[\"checkpoint1\"] = f'./results/{config[\"agent_type\"]}_{config[\"env_type\"]}_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a72c71-8be0-4134-8422-00072b91c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WEAK EXPERIMENT ###\n",
    "for agent_type in [\"TD3\", \"DDPG\", \"DPU\", \"TPS\", \"CDQ\"]:\n",
    "    print(\"AGENT\", agent_type)\n",
    "    config = start_config.copy()\n",
    "    config[\"discount\"] = 0.99\n",
    "    config[\"learning_rate_critic\"] = 0.001\n",
    "    config[\"learning_rate_actor\"] = 0.001\n",
    "    config[\"smoothing_std\"] = 0.0001\n",
    "    config[\"smoothing_clip\"] = 0.0002\n",
    "    config[\"episodes\"] = 1000\n",
    "    config[\"agent_type\"] = agent_type\n",
    "    config[\"env_type\"] = \"hockey\"\n",
    "    config[\"mode\"] = \"weak\"\n",
    "    config[\"name\"] = f'agent_compare_newsmooth_ou'\n",
    "    init_train(config)\n",
    "    config[\"checkpoint1\"] = f'./results/{config[\"agent_type\"]}_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "    config[\"test\"] = True\n",
    "    init_train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55212b8-bfeb-42e7-a082-f19130ad2856",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NORMAL EXPERIMENT ###\n",
    "for agent_type in [\"TD3\", \"DDPG\", \"DPU\", \"TPS\", \"CDQ\"]:\n",
    "    print(\"AGENT\", agent_type)\n",
    "    config = start_config.copy()\n",
    "    config[\"discount\"] = 0.99\n",
    "    config[\"learning_rate_critic\"] = 0.001\n",
    "    config[\"learning_rate_actor\"] = 0.001\n",
    "    config[\"episodes\"] = 1000\n",
    "    config[\"agent_type\"] = agent_type\n",
    "    config[\"env_type\"] = \"hockey\"\n",
    "    config[\"mode\"] = \"normal\"\n",
    "    config[\"name\"] = f'agentcomparefinal'\n",
    "    init_train(config)\n",
    "    config[\"checkpoint1\"] = f'./results/{config[\"agent_type\"]}_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "    config[\"test\"] = True\n",
    "    init_train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b143d058-e5c2-489b-8cf0-85d342cd4467",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFENSE EXPERIMENT ###\n",
    "for agent_type in [\"TD3\", \"DDPG\", \"DPU\", \"TPS\", \"CDQ\"]:\n",
    "    print(\"AGENT\", agent_type)\n",
    "    config = start_config.copy()\n",
    "    config[\"episodes\"] = 1000\n",
    "    config[\"agent_type\"] = agent_type\n",
    "    config[\"env_type\"] = \"hockey\"\n",
    "    config[\"mode\"] = \"defense\"\n",
    "    config[\"name\"] = \"agentcompare_new\"\n",
    "    init_train(config)\n",
    "    config[\"checkpoint1\"] = f'./results/{config[\"agent_type\"]}_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "    config[\"test\"] = True\n",
    "    init_train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6752bfb0-d1c1-4f24-b631-518799543a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ATTACK EXPERIMENT ###\n",
    "for agent_type in [\"DDPG\", \"DPU\", \"TPS\", \"CDQ\", \"TD3\"]:\n",
    "    print(\"AGENT\", agent_type)\n",
    "    config = start_config.copy()\n",
    "    config[\"episodes\"] = 1000\n",
    "    config[\"agent_type\"] = agent_type\n",
    "    config[\"env_type\"] = \"hockey\"\n",
    "    config[\"mode\"] = \"attack\"\n",
    "    config[\"name\"] = f'env_{config[\"env_type\"]}_agent_{agent_type}'\n",
    "    init_train(config)\n",
    "    config[\"checkpoint1\"] = f'./results/{config[\"agent_type\"]}_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea4eba-fa05-4b6b-871d-d47e6a2f1bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING CAMP ###\n",
    "config = start_config.copy()\n",
    "# defense training\n",
    "config[\"name\"] = \"traincamp_new\"\n",
    "config[\"env_type\"] = \"hockey\"\n",
    "config[\"mode\"] = \"defense\"\n",
    "config[\"episodes\"] = 500\n",
    "init_train(config)\n",
    "config[\"checkpoint1\"] = f'./results/{config[\"agent_type\"]}_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "# test agent trained on defense\n",
    "config[\"mode\"] = \"weak\"\n",
    "config[\"test\"] = True\n",
    "init_train(config)\n",
    "\n",
    "# shoot training\n",
    "config[\"mode\"] = \"attack\"\n",
    "config[\"episodes\"] = 500\n",
    "config[\"test\"] = False\n",
    "init_train(config)\n",
    "config[\"checkpoint1\"] = f'./results/{config[\"agent_type\"]}_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "# test agent trained on defense AND shooting\n",
    "config[\"mode\"] = \"weak\"\n",
    "config[\"test\"] = True\n",
    "init_train(config)\n",
    "\n",
    "# regular training\n",
    "config[\"mode\"] = \"weak\"\n",
    "config[\"episodes\"] = 1000\n",
    "config[\"test\"] = False\n",
    "init_train(config)\n",
    "config[\"checkpoint1\"] = f'./results/{config[\"agent_type\"]}_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "# test agent trained on defense AND shooting\n",
    "config[\"mode\"] = \"normal\"\n",
    "config[\"test\"] = True\n",
    "init_train(config)\n",
    "\n",
    "# regular training\n",
    "config[\"mode\"] = \"normal\"\n",
    "config[\"episodes\"] = 1000\n",
    "config[\"test\"] = False\n",
    "init_train(config)\n",
    "config[\"checkpoint1\"] = f'./results/{config[\"agent_type\"]}_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "# test agent trained on defense AND shooting\n",
    "config[\"mode\"] = \"normal\"\n",
    "config[\"test\"] = True\n",
    "init_train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9885a20-f384-483c-ab15-35a8d735f46f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### priority replay !!!NOT USED!!! ###\n",
    "for pri in [True, False]:\n",
    "    config = start_config.copy()\n",
    "    config[\"episodes\"] = 5000\n",
    "    config[\"prio_replay\"] = pri\n",
    "    config[\"name\"] = f\"prio_{pri}\"\n",
    "    config[\"mode\"] = \"normal\"\n",
    "    agent_type=\"TD3\"\n",
    "    env_type=\"hockey\"\n",
    "    init_train(agent_type, env_type, config)\n",
    "    config[\"checkpoint1\"] = f'./results/{agent_type}_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "    # test agent\n",
    "    config[\"mode\"] = \"normal\"\n",
    "    config[\"test\"] = True\n",
    "    #config[\"episodes\"] = 10\n",
    "    #config[\"render\"] = True\n",
    "    init_train(agent_type, env_type, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2bffc2-4fe0-40a6-964a-6bf7ff78a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOT USED ###\n",
    "config = start_config.copy()\n",
    "config[\"episodes\"] = 1000\n",
    "config[\"name\"] = \"20ktest\"\n",
    "for i in range(1):\n",
    "    if i>0:\n",
    "        config[\"mode\"] = \"selfplay\"\n",
    "    config[\"checkpoint1\"] = f'./results/TD3Agent_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "    config[\"checkpoint2\"] = f'./results/TD3Agent_hockey_{config[\"name\"]}_{config[\"mode\"]}_agent.pth'\n",
    "    config[\"name\"] = f\"selfplay{i}\"\n",
    "    config[\"mode\"] = \"selfplay\"\n",
    "    config[\"test\"] = False\n",
    "    init_train(config)\n",
    "    config[\"test\"] = True\n",
    "    config[\"mode\"] = \"normal\"\n",
    "    config[\"checkpoint2\"] = None\n",
    "    init_train(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
